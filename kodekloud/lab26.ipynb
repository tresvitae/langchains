{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87359325",
   "metadata": {},
   "source": [
    "# Lab 26: Complete PDF RAG System - Document Question Answering\n",
    "\n",
    "## Learning Objectives\n",
    "In this lab, you will learn how to:\n",
    "- Build a complete end-to-end RAG (Retrieval Augmented Generation) system for PDF documents\n",
    "- Load and process PDF documents using PyPDFLoader for text extraction\n",
    "- Implement intelligent text chunking strategies for optimal retrieval performance\n",
    "- Create vector embeddings and store them in Chroma vector database\n",
    "- Build a question-answering chain that retrieves relevant context and generates accurate answers\n",
    "- Understand the complete RAG pipeline: Load ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates a production-ready RAG system that can answer questions about PDF documents by combining document retrieval with LLM generation. You'll build a complete pipeline that processes PDF content, creates a searchable knowledge base, and provides accurate answers based on document context. This represents the culmination of all previous RAG concepts in a practical, real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af44e79-f4fd-454e-b8c5-2b0d368be36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG System Implementation - Essential Imports\n",
    "# This lab demonstrates a full end-to-end RAG system for PDF document question answering\n",
    "# combining document loading, processing, embedding, storage, retrieval, and generation\n",
    "\n",
    "# Document Loading and Processing\n",
    "from langchain_community.document_loaders import PyPDFLoader  # PDF document loader for text extraction\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Intelligent text chunking\n",
    "\n",
    "# Vector Storage and Embeddings  \n",
    "from langchain_openai import OpenAIEmbeddings  # High-quality semantic embeddings\n",
    "from langchain_chroma import Chroma  # Vector database for similarity search\n",
    "\n",
    "# LLM and Chain Components\n",
    "from langchain_openai import ChatOpenAI  # OpenAI's chat model for answer generation\n",
    "from langchain.prompts import PromptTemplate  # Structured prompt templates\n",
    "from langchain_core.runnables import RunnablePassthrough  # Data flow management\n",
    "from langchain_core.output_parsers import StrOutputParser  # Clean string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68064f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "# Configure authentication for OpenAI services (embeddings and chat model)\n",
    "import os\n",
    "\n",
    "# Set OpenAI API key for embedding generation and LLM inference\n",
    "# Required for both text-embedding-3-large model and ChatOpenAI model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cc872-b9b8-4ecf-b161-d6aceac0c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PDF Document Loader\n",
    "# PyPDFLoader extracts text content from PDF files while preserving structure\n",
    "# It processes each page separately, maintaining page-level metadata for better document organization\n",
    "\n",
    "# Load the employee handbook PDF document\n",
    "# This could be any PDF: manuals, reports, research papers, documentation, etc.\n",
    "loader = PyPDFLoader(\"data/handbook.pdf\")\n",
    "\n",
    "print(\"üìÑ PDF Loader initialized for: data/handbook.pdf\")\n",
    "print(\"üîß PyPDFLoader will extract text from each page individually\")\n",
    "print(\"üìä Page metadata will be preserved for better document organization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902492c1-61d8-42c7-8c3f-1058b32a57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Extract PDF Content\n",
    "# The load_and_split() method performs two operations:\n",
    "# 1. Extracts text content from all PDF pages\n",
    "# 2. Splits content into individual page documents with metadata\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(f\"üìö Successfully loaded PDF with {len(pages)} pages\")\n",
    "print(\"üîç Each page is now a separate document with preserved metadata\")\n",
    "print(\"üìã Page documents include source file path and page numbers\")\n",
    "\n",
    "# Display information about the loaded content\n",
    "if pages:\n",
    "    print(f\"üìÑ First page preview: {pages[0].page_content[:200]}...\")\n",
    "    print(f\"üè∑Ô∏è Page metadata: {pages[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49023544-d0d1-423d-88e1-6e1d96b0e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Text Chunking for Optimal RAG Performance\n",
    "# RecursiveCharacterTextSplitter creates semantically meaningful chunks for better retrieval\n",
    "\n",
    "# Configure text splitter with optimized parameters:\n",
    "# - chunk_size=200: Small chunks for precise retrieval and focused context\n",
    "# - chunk_overlap=50: 25% overlap ensures context continuity between chunks\n",
    "# This prevents information loss at chunk boundaries and improves retrieval quality\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Split all pages into optimized chunks\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"üì¶ Split {len(pages)} pages into {len(chunks)} chunks\")\n",
    "print(f\"üìè Chunk size: 200 characters with 50-character overlap\")\n",
    "print(f\"üéØ Optimal chunking improves retrieval precision and context relevance\")\n",
    "\n",
    "# Display chunk statistics\n",
    "if chunks:\n",
    "    avg_length = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "    print(f\"üìä Average chunk length: {avg_length:.1f} characters\")\n",
    "    print(f\"üîç Sample chunk: {chunks[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb5fdb-cf75-4bba-bb9b-ffdabff4cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize High-Quality Embedding Model\n",
    "# OpenAI's text-embedding-3-large provides state-of-the-art semantic understanding\n",
    "# Essential for accurate document retrieval in RAG systems\n",
    "\n",
    "# Configure the most advanced OpenAI embedding model:\n",
    "# - 3072 dimensions for rich semantic representation\n",
    "# - Superior performance on similarity and retrieval tasks\n",
    "# - Excellent understanding of domain-specific content\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"üöÄ OpenAI Embeddings initialized with text-embedding-3-large\")\n",
    "print(\"üìê Generates 3072-dimensional vectors for semantic search\")\n",
    "print(\"üéØ Optimized for high-quality document retrieval in RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eba10c-06d7-4a65-9bdf-fbe88cc967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Database from Document Chunks\n",
    "# Chroma.from_documents() automates the embedding and storage process\n",
    "# This creates a searchable knowledge base from the PDF content\n",
    "\n",
    "# The from_documents() method performs multiple operations:\n",
    "# 1. Generates embeddings for each chunk using text-embedding-3-large\n",
    "# 2. Creates Chroma vector database instance\n",
    "# 3. Stores embedded chunks with original text and metadata\n",
    "# 4. Builds similarity search indexes for fast retrieval\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "print(\"üóÑÔ∏è Vector store created successfully!\")\n",
    "print(f\"üìö Embedded and stored {len(chunks)} document chunks\")\n",
    "print(\"üîç Vector database ready for semantic similarity search\")\n",
    "print(\"‚ö° Optimized indexes enable fast retrieval for question answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb0ffa-09d4-4445-b208-342bb8a9a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Document Retriever\n",
    "# Convert vector store into a retriever interface for seamless integration with LangChain\n",
    "# The retriever will find the most relevant document chunks for answering questions\n",
    "\n",
    "# as_retriever() creates a standardized interface that:\n",
    "# - Accepts text queries and converts them to embeddings\n",
    "# - Performs similarity search against stored document vectors\n",
    "# - Returns most relevant chunks ranked by semantic similarity\n",
    "# - Integrates seamlessly with LangChain chains and pipelines\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"üîç Document retriever initialized\")\n",
    "print(\"üìä Retriever will find most relevant chunks for each question\")\n",
    "print(\"üéØ Uses semantic similarity to match questions with document content\")\n",
    "print(\"‚ö° Ready for integration with question-answering chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792988b-c6f1-45fc-a91d-7a7196d50fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Formatting Utility Function\n",
    "# Converts retrieved document chunks into a clean, readable format for the LLM\n",
    "# Essential for preparing context that the language model can effectively process\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents for LLM consumption.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of retrieved document chunks from vector search\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted string with document content separated by double newlines\n",
    "    \"\"\"\n",
    "    # Join document content with clear separators for better LLM comprehension\n",
    "    # Double newlines create clear boundaries between different document chunks\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"üìù Document formatter function defined\")\n",
    "print(\"üîß Converts retrieved chunks into clean context for LLM processing\")\n",
    "print(\"üìã Maintains clear separation between different document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe47c4-a977-418d-b6a8-f5e2f35f8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Language Model for Answer Generation\n",
    "# ChatOpenAI provides the generative component of the RAG system\n",
    "# Will process retrieved context and generate accurate, contextual answers\n",
    "\n",
    "# ChatOpenAI uses GPT-3.5-turbo by default (cost-effective and fast)\n",
    "# The model will:\n",
    "# - Receive structured prompts with context and questions\n",
    "# - Generate answers based only on provided document context\n",
    "# - Maintain factual accuracy by staying within retrieved information\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "print(\"ü§ñ ChatOpenAI language model initialized\")\n",
    "print(\"üí¨ Using GPT-3.5-turbo for answer generation\")\n",
    "print(\"üìö Model will generate answers based on retrieved document context\")\n",
    "print(\"‚úÖ Ready to process questions with factual, context-based responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda1d7a-115a-4fa7-aaca-cf700effbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Structured Prompt Template for RAG Question Answering\n",
    "# The prompt template ensures consistent, accurate responses based on document context\n",
    "# Critical for maintaining factual accuracy and preventing hallucination\n",
    "\n",
    "# Design a prompt template with clear instructions:\n",
    "# 1. Define the AI's role as a factual question-answer bot\n",
    "# 2. Emphasize responding only from provided context\n",
    "# 3. Include fallback behavior for unknown information\n",
    "# 4. Use clear variable placeholders for dynamic content\n",
    "template = \"\"\"SYSTEM: You are a question answer bot. \n",
    "                 Be factual in your response.\n",
    "                 Respond to the following question: {question} only from \n",
    "                 the below context :{context}. \n",
    "                 If you don't know the answer, just say that you don't know.\n",
    "               \"\"\"\n",
    "\n",
    "# Convert template string into a LangChain PromptTemplate object\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(\"üìù RAG prompt template created\")\n",
    "print(\"üéØ Template ensures factual responses based only on document context\") \n",
    "print(\"üö´ Includes safeguards to prevent hallucination and speculation\")\n",
    "print(\"üîß Variables: {question} for user query, {context} for retrieved documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabb299-011d-4bbd-a095-dab874205731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Complete RAG Question-Answering Chain\n",
    "# This chain orchestrates the entire RAG pipeline: Retrieve ‚Üí Format ‚Üí Generate ‚Üí Parse\n",
    "# Demonstrates advanced LangChain composition using LCEL (LangChain Expression Language)\n",
    "\n",
    "# RAG Chain Architecture:\n",
    "# 1. Input: User question\n",
    "# 2. Parallel Processing:\n",
    "#    - retriever | format_docs: Finds relevant chunks and formats them as context\n",
    "#    - RunnablePassthrough(): Passes the original question unchanged\n",
    "# 3. prompt: Combines formatted context and question into structured prompt\n",
    "# 4. llm: Generates answer based on prompt\n",
    "# 5. StrOutputParser(): Extracts clean string response\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚õìÔ∏è Complete RAG chain constructed!\")\n",
    "print(\"üîÑ Pipeline: Question ‚Üí Retrieve ‚Üí Format ‚Üí Generate ‚Üí Parse\")\n",
    "print(\"üìä Parallel processing: Context retrieval + Question passthrough\")\n",
    "print(\"üéØ End-to-end system ready for document-based question answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11fd15-0d78-4d16-b347-0fe9aaddc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute RAG System with Sample Question\n",
    "# Test the complete pipeline with a question about the employee handbook\n",
    "# This demonstrates real-world usage of document-based question answering\n",
    "\n",
    "print(\"‚ùì Testing RAG system with employee handbook question...\")\n",
    "print(\"üîç Question: 'What's the sick leave policy?'\")\n",
    "print(\"üìö System will search through PDF content for relevant information\")\n",
    "print()\n",
    "\n",
    "# Invoke the complete RAG chain:\n",
    "# 1. Retrieves relevant chunks about sick leave policy\n",
    "# 2. Formats context for the language model\n",
    "# 3. Generates accurate answer based on document content\n",
    "# 4. Returns clean, factual response\n",
    "response = chain.invoke(\"What's the sick leave policy?\")\n",
    "\n",
    "print(\"üí¨ RAG System Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"‚úÖ RAG system successfully answered question based on PDF content\")\n",
    "print(\"üéØ Response is grounded in actual document information, not general knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7156ee",
   "metadata": {},
   "source": [
    "## Key Takeaways and Production Insights\n",
    "\n",
    "### What You've Accomplished\n",
    "1. **Complete RAG System**: Built an end-to-end pipeline for PDF document question answering\n",
    "2. **Document Processing**: Implemented intelligent PDF loading and text chunking strategies\n",
    "3. **Vector Storage**: Created searchable knowledge base with high-quality embeddings\n",
    "4. **Intelligent Retrieval**: Built semantic search that finds contextually relevant information\n",
    "5. **Answer Generation**: Combined retrieval with LLM generation for accurate, grounded responses\n",
    "\n",
    "### Technical Architecture\n",
    "- **Document Loading**: PyPDFLoader for robust PDF text extraction with metadata preservation\n",
    "- **Text Processing**: RecursiveCharacterTextSplitter with optimized chunk size (200) and overlap (50)\n",
    "- **Embeddings**: OpenAI text-embedding-3-large for superior semantic understanding\n",
    "- **Vector Database**: Chroma for efficient storage and similarity search\n",
    "- **LLM Integration**: ChatOpenAI with carefully crafted prompts for factual responses\n",
    "\n",
    "### RAG Pipeline Components\n",
    "1. **Load**: Extract content from PDF documents while preserving structure\n",
    "2. **Split**: Create semantically meaningful chunks with optimal size and overlap\n",
    "3. **Embed**: Convert text chunks into high-dimensional vector representations\n",
    "4. **Store**: Build searchable vector database with efficient similarity indexes\n",
    "5. **Retrieve**: Find most relevant chunks based on semantic similarity to questions\n",
    "6. **Generate**: Produce accurate answers grounded in retrieved document context\n",
    "\n",
    "### Production Considerations\n",
    "- **Scalability**: System can handle large document collections with proper infrastructure\n",
    "- **Accuracy**: Factual responses limited to document content prevent hallucination\n",
    "- **Performance**: Optimized chunking and embeddings ensure fast retrieval\n",
    "- **Flexibility**: Can be adapted for various document types and domains\n",
    "\n",
    "### Real-World Applications\n",
    "- **Enterprise Knowledge Bases**: Employee handbooks, policy documents, procedures\n",
    "- **Customer Support**: Product manuals, FAQ systems, troubleshooting guides\n",
    "- **Research Tools**: Academic papers, technical documentation, regulatory documents\n",
    "- **Legal Applications**: Contract analysis, compliance documentation, case law research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
