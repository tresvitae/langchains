{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17bfcea",
   "metadata": {},
   "source": [
    "# Lab 25: Vector Store Implementation with OpenAI Embeddings and Chroma\n",
    "\n",
    "## Learning Objectives\n",
    "In this lab, you will learn how to:\n",
    "- Integrate OpenAI embeddings with Chroma vector database for efficient document storage and retrieval\n",
    "- Create a complete vector store from text documents using automated embedding generation\n",
    "- Perform semantic similarity searches to find relevant documents based on contextual meaning\n",
    "- Understand how vector databases enable intelligent document retrieval for RAG applications\n",
    "- Compare search results for different query types and analyze semantic matching capabilities\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates the practical implementation of a vector store using Chroma database with OpenAI embeddings. You'll learn how to automatically embed documents, store them in a searchable vector database, and perform similarity searches that understand semantic relationships rather than just keyword matching. This foundation is essential for building production-ready RAG systems that can intelligently retrieve relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024b71a-3165-4648-a6e1-7db4606c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries for Vector Store Implementation\n",
    "# This lab demonstrates how to create a complete vector store using OpenAI embeddings and Chroma database\n",
    "# for efficient document storage and semantic similarity search in RAG applications\n",
    "\n",
    "# OpenAIEmbeddings: Converts text into high-dimensional vectors that capture semantic meaning\n",
    "# - Uses OpenAI's text-embedding-3-large model for state-of-the-art embedding quality\n",
    "# - Enables semantic understanding beyond simple keyword matching\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Chroma: Open-source vector database optimized for AI applications\n",
    "# - Provides efficient storage and retrieval of embedded documents\n",
    "# - Supports similarity search with various distance metrics\n",
    "# - Ideal for building RAG systems with fast document retrieval\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "# Set up authentication for OpenAI services to access embedding models\n",
    "# The embedding service requires a valid API key for generating high-quality vector representations\n",
    "\n",
    "import os\n",
    "\n",
    "# Configure OpenAI API key for embedding generation\n",
    "# Replace \"your-api-key\" with your actual OpenAI API key\n",
    "# The embeddings will be generated using OpenAI's text-embedding-3-large model\n",
    "# which provides 3072-dimensional vectors with excellent semantic understanding\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804ab56-f085-44d4-be0c-d82e1c4be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI Embeddings Model\n",
    "# Create an embeddings instance using OpenAI's most advanced embedding model\n",
    "\n",
    "# text-embedding-3-large is OpenAI's highest-quality embedding model\n",
    "# Key features:\n",
    "# - 3072 dimensions for rich semantic representation\n",
    "# - Superior performance on similarity and retrieval tasks\n",
    "# - Optimized for multilingual understanding\n",
    "# - Excellent performance on domain-specific content\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"‚úÖ OpenAI Embeddings initialized with text-embedding-3-large model\")\n",
    "print(\"üìä This model generates 3072-dimensional vectors for semantic search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd63699-b31e-484f-aa7e-9de07a83d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Documents for Vector Store\n",
    "# Define a collection of sports-related documents to demonstrate semantic search capabilities\n",
    "# These documents cover different sports (cricket and football) with varying themes\n",
    "\n",
    "# Sample documents with diverse sports content:\n",
    "# - Cricket World Cup content (documents 1 and 3)\n",
    "# - Football World Cup content (documents 2 and 4)\n",
    "# - Mix of themes: championships, highlights, player stories\n",
    "# This variety will help demonstrate how semantic search finds contextually relevant matches\n",
    "docs = [\n",
    "    \"Thrilling Finale Awaits: The Countdown to the Cricket World Cup Championship\",\n",
    "    \"Global Giants Clash: Football World Cup Semi-Finals Set the Stage for Epic Showdowns\", \n",
    "    \"Record Crowds and Unforgettable Moments: Highlights from the Cricket World Cup\",\n",
    "    \"From Underdogs to Contenders: Football World Cup Surprises and Breakout Stars\"\n",
    "]\n",
    "\n",
    "print(f\"üìÑ Created {len(docs)} sample documents for vector store\")\n",
    "print(\"üèè Documents include cricket and football World Cup content\")\n",
    "print(\"üîç These will be embedded and stored for semantic similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a04fc8-7119-48d2-a779-01bd3890b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Store with Automated Embedding Generation\n",
    "# Use Chroma's from_texts() method to automatically embed and store documents\n",
    "# This combines embedding generation and vector database storage in one operation\n",
    "\n",
    "# Chroma.from_texts() performs several operations:\n",
    "# 1. Generates embeddings for each document using the specified embedding model\n",
    "# 2. Creates a new Chroma vector database instance\n",
    "# 3. Stores the embedded documents with their original text\n",
    "# 4. Creates indexes for efficient similarity search\n",
    "# 5. Returns a searchable vector store ready for queries\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts=docs, embedding=embeddings)\n",
    "\n",
    "print(\"üöÄ Vector store created successfully!\")\n",
    "print(\"üì¶ All documents have been embedded and stored in Chroma database\")\n",
    "print(\"üîç Vector store is ready for similarity search operations\")\n",
    "print(f\"üìä Stored {len(docs)} documents as 3072-dimensional vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ccdf7-4114-4fbd-80a7-068442c909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Semantic Similarity Search - Cricket Context\n",
    "# Search for documents related to \"Rohit Sharma\" (famous cricket player)\n",
    "# This demonstrates how vector search understands semantic relationships\n",
    "\n",
    "# The similarity search process:\n",
    "# 1. Converts query \"Rohit Sharma\" into an embedding vector\n",
    "# 2. Compares this vector against all stored document vectors\n",
    "# 3. Finds documents with highest semantic similarity scores\n",
    "# 4. Returns the most relevant matches based on contextual understanding\n",
    "\n",
    "print(\"üèè Searching for documents related to 'Rohit Sharma' (cricket context)\")\n",
    "print(\"üîç Expected: Cricket-related documents should rank higher due to semantic similarity\")\n",
    "print()\n",
    "\n",
    "results = vectorstore.similarity_search('Rohit Sharma', 2)\n",
    "\n",
    "print(\"üìã Top 2 most similar documents:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    \n",
    "print(\"\\nüí° Notice: Cricket documents are prioritized because Rohit Sharma is a cricket player\")\n",
    "print(\"üß† This demonstrates semantic understanding beyond keyword matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a1738-263c-4372-b315-fd3dc1164057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Semantic Similarity Search - Football Context  \n",
    "# Search for documents related to \"Lionel Messi\" (famous football player)\n",
    "# This demonstrates how the same vector store adapts to different query contexts\n",
    "\n",
    "# Comparing search contexts:\n",
    "# - \"Rohit Sharma\" ‚Üí Cricket association ‚Üí Cricket documents prioritized\n",
    "# - \"Lionel Messi\" ‚Üí Football association ‚Üí Football documents prioritized\n",
    "# This shows how embeddings capture domain-specific semantic relationships\n",
    "\n",
    "print(\"‚öΩ Searching for documents related to 'Lionel Messi' (football context)\")\n",
    "print(\"üîç Expected: Football-related documents should rank higher due to semantic similarity\")\n",
    "print()\n",
    "\n",
    "results = vectorstore.similarity_search('Lionel Messi', 2)\n",
    "\n",
    "print(\"üìã Top 2 most similar documents:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    \n",
    "print(\"\\nüí° Notice: Football documents are prioritized because Lionel Messi is a football player\")\n",
    "print(\"üß† This demonstrates how vector search adapts to different semantic contexts\")\n",
    "print(\"‚öñÔ∏è Compare these results with the Rohit Sharma search to see context-aware retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2e7fe",
   "metadata": {},
   "source": [
    "## Key Takeaways and Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "1. **Vector Store Integration**: Successfully integrated OpenAI embeddings with Chroma database for automated document storage and retrieval\n",
    "2. **Semantic Search Capabilities**: Demonstrated how vector databases understand contextual relationships (cricket vs football contexts)\n",
    "3. **Production-Ready Foundation**: Built a complete vector store system that can scale to thousands of documents\n",
    "4. **Query Adaptation**: Observed how the same vector store intelligently adapts to different query contexts and domains\n",
    "\n",
    "### Technical Insights\n",
    "- **Embedding Quality**: OpenAI's text-embedding-3-large model provides rich 3072-dimensional semantic representations\n",
    "- **Chroma Efficiency**: The `from_texts()` method simplifies the embedding and storage process into a single operation\n",
    "- **Semantic Understanding**: Vector search goes beyond keyword matching to understand meaning and context\n",
    "- **Contextual Retrieval**: Search results adapt based on the semantic domain of the query (sports player associations)\n",
    "\n",
    "### Real-World Applications\n",
    "- **Document Knowledge Bases**: Build intelligent search systems for company documents, manuals, and reports\n",
    "- **Customer Support**: Create semantic search for FAQ systems that understand user intent\n",
    "- **Research Tools**: Enable researchers to find relevant papers and articles based on conceptual similarity\n",
    "- **Content Recommendation**: Develop systems that suggest related content based on semantic relationships\n",
    "\n",
    "### Next Steps for RAG Development\n",
    "1. **Scale Up**: Add hundreds or thousands of documents to test performance at scale\n",
    "2. **Hybrid Search**: Combine semantic search with keyword search for optimal retrieval\n",
    "3. **Metadata Filtering**: Add document metadata for more sophisticated filtering capabilities\n",
    "4. **Integration**: Connect this vector store to LLM chains for complete RAG question-answering systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
