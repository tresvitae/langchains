{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44c42c5",
   "metadata": {},
   "source": [
    "# Lab 12: Streaming LangChain Chain Output\n",
    "\n",
    "This lab demonstrates how to stream responses from a LangChain chain in real-time. You'll learn:\n",
    "- How to use the `stream()` method for real-time output\n",
    "- Benefits of streaming for better user experience\n",
    "- Processing responses as they arrive\n",
    "- Understanding the difference between `invoke()` and `stream()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80f757-fab0-4a97-9d89-249560c97e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain components for streaming demonstration\n",
    "# These are the same components as basic chains, but we'll use streaming functionality\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9edfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key for accessing the language model\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2f159-3120-4660-82c4-22b6b7fe24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the streaming chain\n",
    "# Same structure as basic chains - the streaming happens in execution, not setup\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful assistant. \n",
    "    Answer the following question: {question}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe8064-78d9-4c6d-9fe9-b5b02b35a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model - streaming is supported by default\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012101b-845c-4fd8-afea-698ed6f68f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String output parser works with streaming - it processes each chunk as it arrives\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419f346-4a84-4e4a-b287-329563056abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain - identical to non-streaming chains\n",
    "# The streaming behavior is controlled by how we invoke it\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af008d1c-a8f0-4e26-9b96-05c6aa195381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stream() instead of invoke() to get real-time response chunks\n",
    "# Each chunk is printed as soon as it arrives from the model\n",
    "# This provides better user experience for long responses\n",
    "for chunk in chain.stream({\"question\":\"Tell me about The Godfather Movie\"}):\n",
    "    print(chunk, end=\"\", flush=True)  # Print without newlines for continuous text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
