
lab1 + lab2 - Invoking LLMs
These two labs introduce how to interact with Large Language Models (LLMs) using LangChain. 
- **Lab 1** focuses on the basics of sending a prompt to an LLM and receiving a response, covering the simplest invocation patterns and API usage.
- **Lab 2** builds on Lab 1 by introducing more advanced invocation options, such as customizing model parameters, handling errors, and managing response formats. The key difference is that Lab 2 explores more robust and production-ready ways to invoke LLMs, while Lab 1 is about the fundamentals.

lab3 - Prompt Template
This lab demonstrates how to use prompt templates to structure and reuse prompts efficiently. You will learn how to define templates with variables, fill them dynamically, and understand the benefits of templating for consistency and maintainability in LLM applications.

lab4 - Few-Shot Prompt Template
This lab extends the concept of prompt templates by introducing few-shot learning. You will see how to provide examples within your prompts to guide the LLM towards better and more contextually relevant outputs. The focus is on constructing prompts that include demonstrations for improved model performance.

lab5 + lab6 + lab7 - Parsing Model Output
These three labs cover techniques for parsing and handling the output generated by LLMs.
- **Lab 5** introduces basic parsing strategies, such as extracting structured data from plain text responses.
- **Lab 6** explores more advanced parsing, including error handling and validation of model outputs.
- **Lab 7** focuses on integrating parsing logic into end-to-end workflows, ensuring that model outputs can be reliably used in downstream applications. The progression from Lab 5 to Lab 7 moves from simple extraction to robust, production-grade output handling.

lab8 + lab9 + lab10 - LangChain Debugging and Monitoring
These three labs focus on different approaches to debugging, monitoring, and observing LangChain operations for better development and production insights.
- **Lab 8** demonstrates global debugging using `set_debug(True)` which provides comprehensive tracing of all chain operations including timing, input/output flow, and internal processing steps. It also showcases the modern pipe operator (`|`) syntax for chain composition.
- **Lab 9** focuses on the `verbose=True` parameter in LLMChain, which provides chain-specific logging without global settings. This approach gives you detailed output for individual chains while maintaining control over what gets logged.
- **Lab 10** introduces callback handlers, specifically `StdOutCallbackHandler`, for real-time monitoring of chain execution. This provides the most flexible approach to observability, allowing custom handling of execution events and integration with external monitoring systems. The progression from Lab 8 to Lab 10 moves from global debugging to targeted verbose logging to customizable callback-based monitoring.

lab11 + lab12 + lab13 + lab14 - LangChain Expression Language (LCEL) Fundamentals
These four labs introduce the modern LangChain Expression Language and different execution methods for chains.
- **Lab 11** covers basic chain composition using the pipe operator (`|`) and introduces schema inspection capabilities. You'll learn to build fundamental chains with prompt ‚Üí model ‚Üí parser flow and understand input/output schemas for better debugging and validation.
- **Lab 12** demonstrates streaming functionality using the `stream()` method for real-time response processing. This lab shows how to provide better user experience by displaying responses as they arrive, rather than waiting for complete generation.
- **Lab 13** introduces batch processing with the `batch()` method for efficiently handling multiple queries simultaneously. This approach optimizes performance when processing multiple similar requests.
- **Lab 14** extends batch processing with advanced techniques and best practices for handling different types of batch inputs and optimizing batch operations for better performance and throughput.

lab15 + lab16 - Advanced LangChain Composition Patterns
These two labs explore sophisticated chain composition techniques for building complex workflows.
- **Lab 15** demonstrates custom function integration using `RunnableLambda` to wrap Python functions for use in chains. You'll learn to combine LLM responses with custom logic, create complex processing pipelines, and visualize chain structure with graph representation.
- **Lab 16** showcases advanced chain composition using `RunnablePassthrough` for multi-step content generation workflows. This lab builds a complete content pipeline (input ‚Üí title ‚Üí outline ‚Üí blog ‚Üí summary) demonstrating data flow management between dependent chain operations and creating sophisticated automated content generation systems.

lab17 + lab18 - Memory Management and Configuration
These two labs introduce advanced LangChain features for conversation management and dynamic chain configuration.
- **Lab 17** focuses on implementing short-term memory in conversational AI using `MessagesPlaceholder`. You'll learn how to inject conversation history into prompts, maintain context across multiple interactions, and build AI systems that remember previous exchanges. This lab demonstrates the difference between contextless and contextual conversations, showing how memory enables more natural dialogue flow.
- **Lab 18** demonstrates configurable fields using `ConfigurableField` for dynamic runtime configuration of chain components. You'll learn to create flexible chains that can switch between different models (like GPT-3.5-turbo vs GPT-4) without recreating the entire chain structure. This lab shows how to build adaptable systems that can be reconfigured on-the-fly for different use cases or performance requirements.

lab19 + lab20 - Persistent Memory with Redis
These two labs demonstrate how to implement long-term, persistent memory in conversational AI using Redis as a data store.
- **Lab 19** focuses on setting up Redis-based persistent memory using `RedisChatMessageHistory` and `RunnableWithMessageHistory`. You'll learn how to create conversation threads that survive application restarts, manage multiple independent conversation sessions using session IDs, and understand the advantages of persistent storage over in-memory approaches. This lab shows how to start new conversations and maintain separate context for different topics (math vs physics threads).
- **Lab 20** demonstrates conversation continuity by resuming conversations from Lab 19 after simulating an application restart. You'll see how Redis preserves conversation history across different execution contexts, enabling true stateful conversational applications. This lab proves that conversations can be seamlessly continued from exactly where they left off, making it ideal for production chatbots and conversational AI systems that need to maintain long-term user relationships.

### üîç Redis Database Inspection Guide
To explore and verify the conversation data stored in Redis, you can inspect the database contents using the following commands:

**Step 1: Access the Redis Container**
```bash
docker exec -it <container_id> /bin/bash
```

**Step 2: Connect to Redis CLI**
```bash
redis-cli
```

**Step 3: List All Stored Keys**
```bash
KEYS *
```

**Step 4: View Conversation History**
```bash
# View specific conversation thread (example: math-thread1)
LRANGE message_store:math-thread1 0 -1

# View all messages in a thread from newest to oldest
LRANGE message_store:physics-thread1 0 -1
```

**Additional Useful Redis Commands:**
- `TYPE <key>` - Check the data type of a key
- `TTL <key>` - Check if a key has an expiration time
- `DBSIZE` - Get the total number of keys in the database
- `INFO memory` - Check Redis memory usage statistics 


lab21 + lab22 - Document Loading for RAG (Retrieval Augmented Generation)
These two labs introduce document loading capabilities as the foundation for building RAG systems that can incorporate external knowledge sources.
- **Lab 21** demonstrates PDF document loading using `PyPDFLoader` for extracting and processing text content from PDF files. You'll learn how to load PDF documents, split them into manageable page-based chunks, access individual pages, and understand document structure and metadata. This lab is essential for building knowledge bases from PDF documents like manuals, research papers, and reports.
- **Lab 22** focuses on web content loading using `WebBaseLoader` for extracting text from web pages and online articles. You'll learn how to fetch content from URLs, parse HTML to extract meaningful text, and prepare web data for further processing. This lab enables building RAG systems that can incorporate up-to-date information from websites, news articles, and online documentation, making AI systems more current and comprehensive.

lab23 - Advanced PDF Processing with Text Splitting
This lab extends PDF document processing by introducing intelligent text splitting for optimal RAG performance. You'll learn how to use `RecursiveCharacterTextSplitter` to break down PDF content into semantically meaningful chunks with configurable size and overlap parameters. This lab is crucial for preparing documents for vector databases and similarity search, as proper chunking significantly impacts retrieval quality in RAG systems. You'll understand how to balance chunk size for context preservation while maintaining granularity for precise retrieval, making this essential knowledge for building production-ready knowledge bases.

lab24 - Document Embeddings for Semantic Search
This lab introduces the fundamental concept of document embeddings using OpenAI's embedding models. You'll learn how to convert text documents into high-dimensional vector representations that capture semantic meaning, enabling similarity search and retrieval operations. The lab covers using `OpenAIEmbeddings` with the powerful text-embedding-3-large model, understanding embedding dimensions and vector structure, and preparing embeddings for downstream RAG applications. This is essential groundwork for building semantic search systems, as embeddings form the foundation for finding contextually relevant documents based on meaning rather than just keyword matching.

lab25 - Vector Store Implementation with OpenAI Embeddings and Chroma
This lab demonstrates the complete integration of OpenAI embeddings with Chroma vector database to create a production-ready document storage and retrieval system. You'll learn how to automatically embed multiple documents, store them in a searchable vector database, and perform intelligent similarity searches that understand semantic context rather than just keywords. The lab showcases how the same vector store adapts to different query contexts (cricket vs football) by understanding domain-specific relationships between players and sports. This represents the culmination of RAG foundations, combining embeddings, vector storage, and semantic search into a unified system ready for integration with LLM chains. Essential for building intelligent document retrieval systems, knowledge bases, and context-aware AI applications.

lab26 + lab27 - Complete RAG Systems (PDF and Web Content)
These two labs demonstrate complete end-to-end RAG (Retrieval Augmented Generation) systems that combine all previous concepts into production-ready question-answering applications.
- **Lab 26** builds a comprehensive PDF RAG system using PyPDFLoader, demonstrating the complete pipeline: Load ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate. You'll create a system that can answer questions about employee handbooks, policy documents, or any PDF content with factual accuracy grounded in document context. This lab shows how to process static documents into intelligent knowledge bases for enterprise applications.
- **Lab 27** adapts the same RAG architecture for web content using WebBaseLoader, enabling real-time information retrieval from online sources. You'll build a system that can answer questions about current events, news articles, and frequently-updated web content. This demonstrates RAG system flexibility and shows how the same architectural patterns work across different content sources. Together, these labs provide the complete foundation for building production RAG systems that can handle both static documents and dynamic web content, essential for modern AI applications that need access to both archived knowledge and current information. 

lab28 - Implementing Chains