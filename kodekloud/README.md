
lab1 + lab2 - Invoking LLMs
These two labs introduce how to interact with Large Language Models (LLMs) using LangChain. 
- **Lab 1** focuses on the basics of sending a prompt to an LLM and receiving a response, covering the simplest invocation patterns and API usage.
- **Lab 2** builds on Lab 1 by introducing more advanced invocation options, such as customizing model parameters, handling errors, and managing response formats. The key difference is that Lab 2 explores more robust and production-ready ways to invoke LLMs, while Lab 1 is about the fundamentals.

lab3 - Prompt Template
This lab demonstrates how to use prompt templates to structure and reuse prompts efficiently. You will learn how to define templates with variables, fill them dynamically, and understand the benefits of templating for consistency and maintainability in LLM applications.

lab4 - Few-Shot Prompt Template
This lab extends the concept of prompt templates by introducing few-shot learning. You will see how to provide examples within your prompts to guide the LLM towards better and more contextually relevant outputs. The focus is on constructing prompts that include demonstrations for improved model performance.

lab5 + lab6 + lab7 - Parsing Model Output
These three labs cover techniques for parsing and handling the output generated by LLMs.
- **Lab 5** introduces basic parsing strategies, such as extracting structured data from plain text responses.
- **Lab 6** explores more advanced parsing, including error handling and validation of model outputs.
- **Lab 7** focuses on integrating parsing logic into end-to-end workflows, ensuring that model outputs can be reliably used in downstream applications. The progression from Lab 5 to Lab 7 moves from simple extraction to robust, production-grade output handling.
