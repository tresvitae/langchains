
lab1 + lab2 - Invoking LLMs
These two labs introduce how to interact with Large Language Models (LLMs) using LangChain. 
- **Lab 1** focuses on the basics of sending a prompt to an LLM and receiving a response, covering the simplest invocation patterns and API usage.
- **Lab 2** builds on Lab 1 by introducing more advanced invocation options, such as customizing model parameters, handling errors, and managing response formats. The key difference is that Lab 2 explores more robust and production-ready ways to invoke LLMs, while Lab 1 is about the fundamentals.

lab3 - Prompt Template
This lab demonstrates how to use prompt templates to structure and reuse prompts efficiently. You will learn how to define templates with variables, fill them dynamically, and understand the benefits of templating for consistency and maintainability in LLM applications.

lab4 - Few-Shot Prompt Template
This lab extends the concept of prompt templates by introducing few-shot learning. You will see how to provide examples within your prompts to guide the LLM towards better and more contextually relevant outputs. The focus is on constructing prompts that include demonstrations for improved model performance.

lab5 + lab6 + lab7 - Parsing Model Output
These three labs cover techniques for parsing and handling the output generated by LLMs.
- **Lab 5** introduces basic parsing strategies, such as extracting structured data from plain text responses.
- **Lab 6** explores more advanced parsing, including error handling and validation of model outputs.
- **Lab 7** focuses on integrating parsing logic into end-to-end workflows, ensuring that model outputs can be reliably used in downstream applications. The progression from Lab 5 to Lab 7 moves from simple extraction to robust, production-grade output handling.

lab8 + lab9 + lab10 - LangChain Debugging and Monitoring
These three labs focus on different approaches to debugging, monitoring, and observing LangChain operations for better development and production insights.
- **Lab 8** demonstrates global debugging using `set_debug(True)` which provides comprehensive tracing of all chain operations including timing, input/output flow, and internal processing steps. It also showcases the modern pipe operator (`|`) syntax for chain composition.
- **Lab 9** focuses on the `verbose=True` parameter in LLMChain, which provides chain-specific logging without global settings. This approach gives you detailed output for individual chains while maintaining control over what gets logged.
- **Lab 10** introduces callback handlers, specifically `StdOutCallbackHandler`, for real-time monitoring of chain execution. This provides the most flexible approach to observability, allowing custom handling of execution events and integration with external monitoring systems. The progression from Lab 8 to Lab 10 moves from global debugging to targeted verbose logging to customizable callback-based monitoring. 