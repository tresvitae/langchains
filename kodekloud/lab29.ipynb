{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d1a793",
   "metadata": {},
   "source": [
    "# Lab 29: Advanced Retrieval Chains - Integrated RAG with Chain Abstractions\n",
    "\n",
    "## Learning Objectives\n",
    "In this lab, you will learn how to:\n",
    "- Build advanced retrieval chains using LangChain's high-level chain abstractions\n",
    "- Combine document processing chains with retrieval systems for scalable RAG\n",
    "- Use FAISS vector store for high-performance similarity search\n",
    "- Implement `create_retrieval_chain` for integrated question-answering workflows\n",
    "- Compare different vector store implementations (FAISS vs Chroma)\n",
    "- Build production-ready RAG systems using chain composition patterns\n",
    "- Process multiple web sources with intelligent retrieval and generation\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates advanced RAG implementation using LangChain's high-level chain abstractions, specifically `create_retrieval_chain` and `create_stuff_documents_chain`. You'll learn how to build sophisticated question-answering systems that combine the power of semantic retrieval with document processing chains. This approach provides a more structured and maintainable way to build RAG systems compared to manual chain composition, while offering better performance through FAISS vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ea211-6277-46e3-aaca-3b4c3520c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Retrieval Chain Implementation - Complete RAG Stack\n",
    "# This lab demonstrates sophisticated RAG systems using LangChain's high-level chain abstractions\n",
    "# combining retrieval, document processing, and generation in an integrated workflow\n",
    "\n",
    "# Core LangChain Components\n",
    "from langchain_core.prompts import PromptTemplate  # Structured prompt templates\n",
    "from langchain_openai import ChatOpenAI  # OpenAI chat model for generation\n",
    "from langchain_openai import OpenAIEmbeddings  # High-quality embeddings\n",
    "\n",
    "# Document Loading and Processing\n",
    "from langchain_community.document_loaders import WebBaseLoader  # Multi-URL web content\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Intelligent chunking\n",
    "\n",
    "# Advanced Chain Abstractions\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Document processing\n",
    "from langchain.chains import create_retrieval_chain  # Integrated retrieval + generation\n",
    "\n",
    "# High-Performance Vector Store\n",
    "from langchain_community.vectorstores import FAISS  # Facebook AI Similarity Search\n",
    "\n",
    "print(\"üöÄ Advanced RAG stack components imported\")\n",
    "print(\"‚ö° Features: High-level chain abstractions + FAISS vector store\")\n",
    "print(\"üéØ Goal: Production-ready retrieval chain architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "# Configure authentication for embeddings and chat model\n",
    "import os\n",
    "\n",
    "# Set OpenAI API key for advanced RAG pipeline\n",
    "# Required for text-embedding-3-large and GPT-3.5-turbo models\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2e244-c486-4b93-8785-c0f0c64a73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URLs for Advanced RAG System\n",
    "# Multiple TechCrunch articles about different AI companies and their model developments\n",
    "# Ideal for testing retrieval capabilities across diverse AI company information\n",
    "\n",
    "# URL 1: Anthropic's Claude models vs GPT-4 comparison\n",
    "# Contains information about model capabilities, performance benchmarks, and technical details\n",
    "URL1 = \"https://techcrunch.com/2024/03/04/anthropic-claims-its-new-models-beat-gpt-4/\"\n",
    "\n",
    "# URL 2: AI21 Labs' efficient text generation model\n",
    "# Covers model efficiency, technical specifications, and performance characteristics\n",
    "URL2 = \"https://techcrunch.com/2024/03/28/ai21-labs-new-text-generating-ai-model-is-more-efficient-than-most/\"\n",
    "\n",
    "print(\"üåê Advanced RAG content sources configured:\")\n",
    "print(f\"üì∞ Article 1: Anthropic's models vs GPT-4 performance\")\n",
    "print(f\"üì∞ Article 2: AI21 Labs' efficient model architecture\")\n",
    "print(\"üîç Perfect for testing cross-company model information retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006549bd-9351-451a-8855-2947d3474443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multiple Web Documents for RAG Knowledge Base\n",
    "# WebBaseLoader processes multiple AI news articles to create a diverse knowledge base\n",
    "# This provides rich content for testing advanced retrieval capabilities\n",
    "\n",
    "# Multi-document loading for RAG:\n",
    "# 1. Fetches content from both TechCrunch articles about different AI companies\n",
    "# 2. Extracts clean text content from HTML structure\n",
    "# 3. Preserves source metadata for answer attribution\n",
    "# 4. Creates document objects ready for chunking and embedding\n",
    "loader = WebBaseLoader([URL1, URL2])\n",
    "data = loader.load()\n",
    "\n",
    "print(f\"üìö Successfully loaded {len(data)} AI industry documents\")\n",
    "print(\"ü§ñ Content covers: Anthropic Claude models + AI21 Labs efficiency\")\n",
    "print(\"üìä Rich knowledge base for cross-company model comparisons\")\n",
    "\n",
    "# Display loaded document statistics\n",
    "total_chars = sum(len(doc.page_content) for doc in data)\n",
    "print(f\"üìÑ Total content: {total_chars} characters\")\n",
    "for i, doc in enumerate(data, 1):\n",
    "    print(f\"  Document {i}: {len(doc.page_content)} chars from {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369aa37-9c3f-44f0-8402-0b4d293b420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Text Chunking for Advanced RAG Performance\n",
    "# RecursiveCharacterTextSplitter optimizes content for both retrieval accuracy and context preservation\n",
    "# Critical for ensuring high-quality semantic search and answer generation\n",
    "\n",
    "# Optimal chunking parameters for AI news content:\n",
    "# - chunk_size=200: Small chunks for precise retrieval of specific model information\n",
    "# - chunk_overlap=50: 25% overlap maintains context continuity between chunks\n",
    "# This configuration excels at finding specific technical details while preserving context\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Split AI industry documents into semantically coherent chunks\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"üì¶ Created {len(chunks)} optimized chunks from {len(data)} documents\")\n",
    "print(f\"üìè Chunk configuration: 200 characters with 50-character overlap\")\n",
    "print(f\"üéØ Optimized for precise retrieval of AI model specifications\")\n",
    "\n",
    "# Analyze chunking results\n",
    "if chunks:\n",
    "    avg_length = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "    print(f\"üìä Average chunk length: {avg_length:.1f} characters\")\n",
    "    print(f\"üîç Sample chunk preview: {chunks[0].page_content[:100]}...\")\n",
    "    print(f\"üè∑Ô∏è Chunk metadata preserved: {list(chunks[0].metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690ebb5-97e0-4878-8214-99cc53b5ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Advanced Embedding Model for Semantic Search\n",
    "# OpenAI's text-embedding-3-large provides state-of-the-art semantic understanding\n",
    "# Essential for accurate retrieval in sophisticated RAG systems\n",
    "\n",
    "# text-embedding-3-large advantages for AI content:\n",
    "# - 3072 dimensions for rich semantic representation\n",
    "# - Superior performance on technical and domain-specific content\n",
    "# - Excellent at understanding AI/ML terminology and concepts\n",
    "# - Optimized for similarity search in specialized domains\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"üöÄ OpenAI text-embedding-3-large initialized\")\n",
    "print(\"üìê Generates 3072-dimensional vectors for semantic search\")\n",
    "print(\"üß† Optimized for AI/ML domain content understanding\")\n",
    "print(\"‚ö° Ready for high-performance vector storage with FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14338f-6c3d-4323-b790-8a2f7582153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create High-Performance Vector Store with FAISS\n",
    "# FAISS (Facebook AI Similarity Search) provides enterprise-grade vector search capabilities\n",
    "# Superior performance compared to Chroma for production RAG systems\n",
    "\n",
    "# FAISS advantages:\n",
    "# - Optimized for large-scale similarity search\n",
    "# - Multiple index types for different performance characteristics\n",
    "# - Memory-efficient storage and retrieval\n",
    "# - Battle-tested in production environments\n",
    "# - Excellent performance with OpenAI embeddings\n",
    "\n",
    "# Create FAISS vector store from document chunks\n",
    "vector = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Initialize retriever interface for seamless chain integration\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "print(\"üóÑÔ∏è FAISS vector store created successfully!\")\n",
    "print(f\"üìö Embedded and indexed {len(chunks)} AI content chunks\")\n",
    "print(\"‚ö° High-performance similarity search ready\")\n",
    "print(\"üîç Retriever interface configured for chain integration\")\n",
    "print(\"üìä FAISS optimized for enterprise-scale vector operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9a2a3-c184-4fde-a20d-c9413cea201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Advanced Prompt Template for RAG Question Answering\n",
    "# Structured prompt design for accurate, context-grounded responses\n",
    "# Optimized for technical AI content with specific information extraction\n",
    "\n",
    "# Advanced prompt template features:\n",
    "# - Clear context boundaries with XML-style tags\n",
    "# - Explicit fallback behavior for unknown information\n",
    "# - Structured variable placeholders for clean formatting\n",
    "# - Designed to prevent hallucination and ensure factual accuracy\n",
    "prompt_template = \"\"\"\n",
    "    Answer the question {input} based solely on the context below:\n",
    "    \\n\\n'<context>\\n{context}\\n</context>'\n",
    "    If you can't find an answer, say I don't know.\n",
    "    \"\"\"\n",
    "\n",
    "# Convert to LangChain PromptTemplate object\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "print(\"üìù Advanced RAG prompt template created\")\n",
    "print(\"üéØ Optimized for technical AI content question answering\")\n",
    "print(\"üö´ Built-in safeguards prevent hallucination and speculation\")\n",
    "print(\"üîß Variables: {input} for questions, {context} for retrieved content\")\n",
    "print(\"üìã XML-style context tags for clear boundary definition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cfce0-c908-477b-9ef0-087dd842f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Language Model for Advanced RAG Generation\n",
    "# ChatOpenAI with optimized configuration for factual, technical content generation\n",
    "# Temperature set to 0.0 for consistent, deterministic responses\n",
    "\n",
    "# Model configuration for RAG:\n",
    "# - gpt-3.5-turbo: Cost-effective, high-quality model for question answering\n",
    "# - temperature=0.0: Deterministic output, reduces hallucination risk\n",
    "# - Optimized for processing technical AI content and model specifications\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "print(\"ü§ñ ChatOpenAI configured for advanced RAG generation\")\n",
    "print(\"üí¨ Model: GPT-3.5-turbo with temperature=0.0\")\n",
    "print(\"üéØ Optimized for: Factual, deterministic responses\")\n",
    "print(\"üìä Ready for processing AI industry technical content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d2f71-03ad-4161-b0c9-c9ca7bf8a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Advanced Document Processing Chain\n",
    "# create_stuff_documents_chain builds the document processing component of the RAG pipeline\n",
    "# This chain handles the generation phase after retrieval has provided relevant context\n",
    "\n",
    "# Document processing chain architecture:\n",
    "# 1. Receives retrieved document chunks as context\n",
    "# 2. Combines context with user question using prompt template\n",
    "# 3. Sends structured prompt to language model\n",
    "# 4. Returns generated answer based on retrieved information\n",
    "\n",
    "# \"Stuff\" strategy benefits:\n",
    "# - Simple and reliable for most RAG use cases\n",
    "# - All retrieved documents processed together\n",
    "# - Maintains context relationships between chunks\n",
    "# - Optimal for precise, fact-based question answering\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "print(\"‚õìÔ∏è Document processing chain created!\")\n",
    "print(\"üìÑ Type: Stuff documents chain for context combination\")\n",
    "print(\"üîß Architecture: Retrieved Context + Question ‚Üí Prompt ‚Üí LLM ‚Üí Answer\")\n",
    "print(\"üéØ Optimized for factual AI industry question answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843743f-e6e3-4ecc-8227-3c9d90e8f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Advanced Retrieval Chain - Complete RAG Integration\n",
    "# create_retrieval_chain combines retrieval and generation into a unified, high-performance pipeline\n",
    "# This is the pinnacle of LangChain's RAG abstractions for production systems\n",
    "\n",
    "# Advanced retrieval chain architecture:\n",
    "# 1. Question Input: Receives user questions about AI models and companies\n",
    "# 2. Semantic Retrieval: Uses FAISS + embeddings to find relevant chunks\n",
    "# 3. Context Preparation: Formats retrieved chunks for optimal LLM processing\n",
    "# 4. Document Processing: Applies create_stuff_documents_chain for generation\n",
    "# 5. Answer Generation: Returns comprehensive, grounded responses\n",
    "\n",
    "# Integration benefits:\n",
    "# - Seamless retrieval-to-generation workflow\n",
    "# - Automatic context management and formatting\n",
    "# - Built-in error handling and optimization\n",
    "# - Production-ready performance and reliability\n",
    "chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "print(\"üöÄ Advanced retrieval chain constructed!\")\n",
    "print(\"‚õìÔ∏è Complete pipeline: Question ‚Üí Retrieve ‚Üí Process ‚Üí Generate\")\n",
    "print(\"üìä Integration: FAISS retrieval + Document processing + LLM generation\")\n",
    "print(\"üéØ Production-ready RAG system for AI industry knowledge\")\n",
    "print(\"‚ö° Optimized for scalable, accurate question answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cfc22-30bd-4852-8295-ea62e399378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Advanced RAG System with Complex Query\n",
    "# Test the complete retrieval chain with a sophisticated question requiring cross-company analysis\n",
    "# Demonstrates the system's ability to synthesize information from multiple sources\n",
    "\n",
    "print(\"üîç Testing advanced RAG system with complex technical query...\")\n",
    "print(\"‚ùì Question: 'List the models and their token size of models only from Anthropic and Meta'\")\n",
    "print(\"üìä This requires:\")\n",
    "print(\"  - Filtering information by specific companies (Anthropic and Meta)\")\n",
    "print(\"  - Extracting model names from retrieved content\")\n",
    "print(\"  - Finding token size specifications\")\n",
    "print(\"  - Organizing results in a structured format\")\n",
    "print()\n",
    "\n",
    "# Advanced retrieval chain execution:\n",
    "# 1. Embeds the complex query using text-embedding-3-large\n",
    "# 2. Performs semantic search across AI industry documents\n",
    "# 3. Retrieves most relevant chunks about Anthropic and Meta models\n",
    "# 4. Processes context through document chain with structured prompt\n",
    "# 5. Generates comprehensive answer with model specifications\n",
    "result = chain.invoke({\"input\": \"List the models and their token size of models only from Anthropic and Meta\"})\n",
    "\n",
    "print(\"üìã Advanced RAG System Response Processing Complete\")\n",
    "print(\"‚úÖ Successfully retrieved and processed multi-company model information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b47406-573d-4e55-b119-1fa24a47297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Advanced RAG System Results\n",
    "# Show the comprehensive answer generated by the retrieval chain\n",
    "# Demonstrates the system's ability to synthesize complex technical information\n",
    "\n",
    "print(\"üí¨ Advanced RAG System Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Additional result analysis\n",
    "print(\"üìä Result Analysis:\")\n",
    "print(f\"üîç Retrieved Context Length: {len(result.get('context', []))} chunks\")\n",
    "print(f\"üìÑ Answer Length: {len(result['answer'])} characters\")\n",
    "print()\n",
    "print(\"‚úÖ Advanced RAG System Performance Summary:\")\n",
    "print(\"  üéØ Successfully filtered by specific companies (Anthropic and Meta)\")\n",
    "print(\"  üìã Extracted structured model information from unstructured content\")\n",
    "print(\"  üîç Demonstrated cross-document information synthesis\")\n",
    "print(\"  ‚ö° Leveraged FAISS for high-performance semantic retrieval\")\n",
    "print(\"  üß† Used advanced embeddings for precise technical content matching\")\n",
    "print()\n",
    "print(\"üöÄ This demonstrates production-ready RAG capabilities for:\")\n",
    "print(\"  - Technical documentation analysis\")\n",
    "print(\"  - Multi-source information synthesis\")\n",
    "print(\"  - Structured data extraction from unstructured content\")\n",
    "print(\"  - Enterprise knowledge base question answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f93cb9",
   "metadata": {},
   "source": [
    "## Key Takeaways and Advanced RAG Architecture\n",
    "\n",
    "### What You've Accomplished\n",
    "1. **Advanced Retrieval Chain**: Built sophisticated RAG system using high-level LangChain abstractions\n",
    "2. **FAISS Integration**: Implemented enterprise-grade vector search for superior performance\n",
    "3. **Chain Composition**: Combined retrieval and document processing chains seamlessly\n",
    "4. **Complex Querying**: Handled multi-criteria questions requiring information synthesis\n",
    "5. **Production Architecture**: Created scalable, maintainable RAG system design\n",
    "\n",
    "### Technical Architecture Evolution\n",
    "\n",
    "| Lab | Architecture | Components | Use Case |\n",
    "|-----|-------------|------------|----------|\n",
    "| **Lab 26-27** | Manual RAG | Custom LCEL chains | Learning RAG fundamentals |\n",
    "| **Lab 28** | Document Chain | Simple document processing | Known document sets |\n",
    "| **Lab 29** | Advanced RAG | High-level chain abstractions | Production RAG systems |\n",
    "\n",
    "### Advanced Features Introduced\n",
    "\n",
    "#### FAISS vs Chroma Comparison\n",
    "| Aspect | FAISS (Lab 29) | Chroma (Labs 25-27) |\n",
    "|--------|----------------|---------------------|\n",
    "| **Performance** | Optimized for large-scale | Good for prototyping |\n",
    "| **Memory Usage** | Highly efficient | Standard efficiency |\n",
    "| **Production Ready** | Enterprise-grade | Development-friendly |\n",
    "| **Scalability** | Excellent | Good |\n",
    "| **Index Types** | Multiple algorithms | Standard similarity |\n",
    "\n",
    "#### Chain Abstraction Benefits\n",
    "- **create_retrieval_chain**: Integrated retrieval + generation workflow\n",
    "- **create_stuff_documents_chain**: Optimized document processing\n",
    "- **Automatic Error Handling**: Built-in robustness and error management\n",
    "- **Performance Optimization**: Internal optimizations for speed and efficiency\n",
    "- **Maintenance Simplicity**: Higher-level abstractions reduce complexity\n",
    "\n",
    "### Production RAG System Characteristics\n",
    "1. **Scalability**: FAISS handles millions of vectors efficiently\n",
    "2. **Reliability**: High-level chains provide robust error handling\n",
    "3. **Maintainability**: Abstract interfaces simplify development and updates\n",
    "4. **Performance**: Optimized for production workloads\n",
    "5. **Flexibility**: Easy to extend and customize for specific domains\n",
    "\n",
    "### Real-World Applications\n",
    "- **Enterprise Knowledge Bases**: Company documentation and policies\n",
    "- **Technical Support**: Product manuals and troubleshooting guides\n",
    "- **Research Platforms**: Academic papers and technical documentation\n",
    "- **Market Intelligence**: Industry reports and competitive analysis\n",
    "- **Legal Research**: Case law and regulatory documentation\n",
    "\n",
    "### Development Best Practices\n",
    "- **Use High-Level Abstractions**: Prefer chain factories over manual composition\n",
    "- **Choose Appropriate Vector Stores**: FAISS for production, Chroma for prototyping\n",
    "- **Optimize Chunking Strategy**: Balance context preservation with retrieval precision\n",
    "- **Design Robust Prompts**: Include clear instructions and fallback behaviors\n",
    "- **Test with Complex Queries**: Validate system performance with real-world questions\n",
    "\n",
    "### Migration Path\n",
    "1. **Start with Document Chains** (Lab 28) for simple use cases\n",
    "2. **Learn Manual RAG** (Labs 26-27) to understand fundamentals  \n",
    "3. **Adopt Advanced Chains** (Lab 29) for production systems\n",
    "4. **Scale with FAISS** for large document collections\n",
    "5. **Customize as Needed** for domain-specific requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
